{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnG8uRdCh8Na"
      },
      "source": [
        "# Modelo B para clasificación de imagenes\n",
        "\n",
        "#### Jeffrey Daniel Leiva Cascante 2021016720\n",
        "#### Richard Osvaldo León Chinchilla carnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zcmg7q7Fh8Nv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNuSZWCkD8cL",
        "outputId": "96ed6012-ca63-4850-ca5b-2f79dc659b63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Propuesta de modelo 1"
      ],
      "metadata": {
        "id": "4s3Y5hHrvgzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionModuleOne(nn.Module):\n",
        "  def __init__(self, chanels):\n",
        "    super(InceptionModuleOne, self).__init__()\n",
        "\n",
        "    self.branch1 = nn.Conv2d(chanels, 128, kernel_size=1)\n",
        "\n",
        "    self.branch2 = nn.Sequential(\n",
        "        nn.Conv2d(chanels, 128, kernel_size=1),\n",
        "        nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "    )\n",
        "\n",
        "    self.branch3 = nn.Sequential(\n",
        "        nn.Conv2d(chanels, 128, kernel_size=1),\n",
        "        nn.Conv2d(128, 128, kernel_size=5, padding=2)\n",
        "    )\n",
        "\n",
        "    self.branch4 = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "        nn.Conv2d(chanels, 128, kernel_size=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    branch1 = self.branch1(x)\n",
        "    branch2 = self.branch2(x)\n",
        "    branch3 = self.branch3(x)\n",
        "    branch4 = self.branch4(x)\n",
        "    return torch.cat([branch1, branch2, branch3, branch4],1)\n",
        "\n",
        "\n",
        "class CNNModelBOne(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNModelBOne, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.inception = InceptionModuleOne(64)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.fc1 = nn.Linear(512 *32 *32, 128)\n",
        "    self.fc2 = nn.Linear(128 , 3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.inception(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.dropout(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "jTjOohHDvb_Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proof of Concept modelo 1"
      ],
      "metadata": {
        "id": "12nJbPdVFopw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Redimensionar a 128x128\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convertir a escala de grises\n",
        "    transforms.RandomRotation(20),  # Rotación aleatoria\n",
        "    transforms.RandomHorizontalFlip(),  # Inversión horizontal aleatoria\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Variación de brillo y contraste\n",
        "    transforms.ToTensor(),  # Convertir a tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalización\n",
        "])\n",
        "\n",
        "\n",
        "test_data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Redimensionar a 128x128\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convertir a escala de grises\n",
        "    transforms.ToTensor(),   # Convertir a tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalización\n",
        "])\n",
        "\n",
        "\n",
        "# Definir las rutas a las carpetas\n",
        "train_dir = '/content/drive/MyDrive/Covid19-dataset/train'\n",
        "test_dir = '/content/drive/MyDrive/Covid19-dataset/test'\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_data_transforms)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_data_transforms)\n",
        "\n",
        "# Definir el tamaño del subconjunto que quieres usar\n",
        "subset_size = 0.35  # Por ejemplo, usar el 20% de los datos\n",
        "\n",
        "# Obtener los índices aleatorios para el subconjunto del conjunto de entrenamiento\n",
        "train_size = len(train_dataset)\n",
        "indices = list(range(train_size))\n",
        "np.random.shuffle(indices)  # Mezclar los índices\n",
        "\n",
        "subset_indices = indices[:int(subset_size * train_size)]  # Obtener el subconjunto\n",
        "\n",
        "# Crear un SubsetRandomSampler con los índices seleccionados\n",
        "train_sampler = SubsetRandomSampler(subset_indices)\n",
        "\n",
        "# Obtener los índices aleatorios para el subconjunto del conjunto de testing\n",
        "test_size = len(test_dataset)\n",
        "indices_test = list(range(test_size))\n",
        "np.random.shuffle(indices_test)  # Mezclar los índices\n",
        "\n",
        "subset_indices_test = indices_test[:int(subset_size * test_size)]  # Obtener el subconjunto\n",
        "\n",
        "# Crear un SubsetRandomSampler con los índices seleccionados\n",
        "test_sampler = SubsetRandomSampler(subset_indices_test)\n",
        "\n",
        "# Definir DataLoader para el conjunto de entrenamiento con el subconjunto de datos\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
        "\n",
        "# También puedes crear un DataLoader para el conjunto de prueba completo\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, sampler=test_sampler)\n",
        "\n",
        "\n",
        "model = CNNModelBOne()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Definir la función de entrenamiento\n",
        "def train_model_with_subset(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Poner el modelo en modo de entrenamiento\n",
        "\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Training\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Limpiar los gradientes\n",
        "            outputs = model(inputs)  # Hacer una predicción\n",
        "            loss = criterion(outputs, labels)  # Calcular la pérdida\n",
        "            loss.backward()  # Hacer backpropagation\n",
        "            optimizer.step()  # Actualizar los pesos\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = val_running_loss / len(test_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Train the model with validation\n",
        "train_model_with_subset(model, train_loader, test_loader, criterion, optimizer, epochs=30)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdbfJ79pFqrz",
        "outputId": "881f7e9e-f812-4bcb-c250-9278922fcb69"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 1.0804, Train Accuracy: 41.38%, Val Loss: 0.9914, Val Accuracy: 73.91%\n",
            "Epoch 2/30, Train Loss: 0.9736, Train Accuracy: 64.37%, Val Loss: 0.8126, Val Accuracy: 78.26%\n",
            "Epoch 3/30, Train Loss: 0.7739, Train Accuracy: 71.26%, Val Loss: 0.7246, Val Accuracy: 69.57%\n",
            "Epoch 4/30, Train Loss: 0.6224, Train Accuracy: 73.56%, Val Loss: 0.5346, Val Accuracy: 86.96%\n",
            "Epoch 5/30, Train Loss: 0.5134, Train Accuracy: 79.31%, Val Loss: 0.5134, Val Accuracy: 82.61%\n",
            "Epoch 6/30, Train Loss: 0.4175, Train Accuracy: 83.91%, Val Loss: 0.4714, Val Accuracy: 78.26%\n",
            "Epoch 7/30, Train Loss: 0.3786, Train Accuracy: 87.36%, Val Loss: 0.4707, Val Accuracy: 73.91%\n",
            "Epoch 8/30, Train Loss: 0.3475, Train Accuracy: 83.91%, Val Loss: 0.3871, Val Accuracy: 78.26%\n",
            "Epoch 9/30, Train Loss: 0.3094, Train Accuracy: 87.36%, Val Loss: 0.3704, Val Accuracy: 86.96%\n",
            "Epoch 10/30, Train Loss: 0.2809, Train Accuracy: 89.66%, Val Loss: 0.4215, Val Accuracy: 78.26%\n",
            "Epoch 11/30, Train Loss: 0.2846, Train Accuracy: 88.51%, Val Loss: 0.3828, Val Accuracy: 73.91%\n",
            "Epoch 12/30, Train Loss: 0.2442, Train Accuracy: 89.66%, Val Loss: 0.4288, Val Accuracy: 73.91%\n",
            "Epoch 13/30, Train Loss: 0.2415, Train Accuracy: 93.10%, Val Loss: 0.3536, Val Accuracy: 78.26%\n",
            "Epoch 14/30, Train Loss: 0.2156, Train Accuracy: 94.25%, Val Loss: 0.3468, Val Accuracy: 82.61%\n",
            "Epoch 15/30, Train Loss: 0.2474, Train Accuracy: 90.80%, Val Loss: 0.3773, Val Accuracy: 78.26%\n",
            "Epoch 16/30, Train Loss: 0.2081, Train Accuracy: 93.10%, Val Loss: 0.3458, Val Accuracy: 82.61%\n",
            "Epoch 17/30, Train Loss: 0.1916, Train Accuracy: 91.95%, Val Loss: 0.3431, Val Accuracy: 82.61%\n",
            "Epoch 18/30, Train Loss: 0.1967, Train Accuracy: 93.10%, Val Loss: 0.3404, Val Accuracy: 82.61%\n",
            "Epoch 19/30, Train Loss: 0.1784, Train Accuracy: 93.10%, Val Loss: 0.3213, Val Accuracy: 86.96%\n",
            "Epoch 20/30, Train Loss: 0.2307, Train Accuracy: 90.80%, Val Loss: 0.3267, Val Accuracy: 86.96%\n",
            "Epoch 21/30, Train Loss: 0.1717, Train Accuracy: 94.25%, Val Loss: 0.3620, Val Accuracy: 82.61%\n",
            "Epoch 22/30, Train Loss: 0.1775, Train Accuracy: 91.95%, Val Loss: 0.3125, Val Accuracy: 91.30%\n",
            "Epoch 23/30, Train Loss: 0.1690, Train Accuracy: 94.25%, Val Loss: 0.3195, Val Accuracy: 91.30%\n",
            "Epoch 24/30, Train Loss: 0.1700, Train Accuracy: 93.10%, Val Loss: 0.3187, Val Accuracy: 86.96%\n",
            "Epoch 25/30, Train Loss: 0.1838, Train Accuracy: 96.55%, Val Loss: 0.3219, Val Accuracy: 86.96%\n",
            "Epoch 26/30, Train Loss: 0.1705, Train Accuracy: 94.25%, Val Loss: 0.3193, Val Accuracy: 82.61%\n",
            "Epoch 27/30, Train Loss: 0.1612, Train Accuracy: 94.25%, Val Loss: 0.3126, Val Accuracy: 86.96%\n",
            "Epoch 28/30, Train Loss: 0.1827, Train Accuracy: 91.95%, Val Loss: 0.3314, Val Accuracy: 82.61%\n",
            "Epoch 29/30, Train Loss: 0.1563, Train Accuracy: 93.10%, Val Loss: 0.3804, Val Accuracy: 82.61%\n",
            "Epoch 30/30, Train Loss: 0.1355, Train Accuracy: 94.25%, Val Loss: 0.3343, Val Accuracy: 82.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Listar el contenido de la carpeta Covid19\n",
        "os.listdir('/content/drive/MyDrive/Covid19-dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR0ZHmsGH-np",
        "outputId": "8311b11b-3316-4dab-80b6-2de6e7d8a526"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}